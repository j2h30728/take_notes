from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st

# Streamlit ì•± ì„¤ì • (í˜ì´ì§€ ì œëª©, ì•„ì´ì½˜ ì„¤ì •)
st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)

# Callback handler í´ë˜ìŠ¤ ì •ì˜: AI ì‘ë‹µì´ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë˜ë„ë¡ í•¸ë“¤ë§
class ChatCallbackHandler(BaseCallbackHandler):
    message = ""  # ë©”ì‹œì§€ ì´ˆê¸°í™”

    def on_llm_start(self, *args, **kwargs):
        # ìƒˆë¡œìš´ ë©”ì‹œì§€ ë°•ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ë¹„ì›Œë‘ 
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        # AI ì‘ë‹µ ì™„ë£Œ ì‹œ ë©”ì‹œì§€ë¥¼ ì €ì¥
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        # ìƒˆë¡œìš´ í† í°ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ ë©”ì‹œì§€ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
        self.message += token
        self.message_box.markdown(self.message)

# OpenAIì˜ LLM(ì–¸ì–´ ëª¨ë¸) ìƒì„±, ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ê³¼ í•¸ë“¤ëŸ¬ ë“±ë¡
llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    callbacks=[
        ChatCallbackHandler(),  # ìœ„ì—ì„œ ì •ì˜í•œ í•¸ë“¤ëŸ¬
    ],
)

# íŒŒì¼ì„ ì„ë² ë”©í•˜ê³  ë²¡í„°í™”í•˜ëŠ” í•¨ìˆ˜
@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()  # íŒŒì¼ ë‚´ìš©ì„ ì½ìŒ
    file_path = f"./.cache/files/{file.name}"  # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    with open(file_path, "wb") as f:
        f.write(file_content)  # íŒŒì¼ì„ ì§€ì •ëœ ê²½ë¡œì— ì €ì¥

    # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì • (í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ”)
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )

    # íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¶„í• 
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)

    # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    
    # ì„ë² ë”© ë° ìºì‹œ ì„¤ì •
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    # ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)

    # ê²€ìƒ‰ê¸° ìƒì„± ë° ë°˜í™˜
    retriever = vectorstore.as_retriever()
    return retriever

# ì±„íŒ… ë©”ì‹œì§€ ì €ì¥ í•¨ìˆ˜
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

# ì±„íŒ… ë©”ì‹œì§€ë¥¼ UIì— í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
def send_message(message, role, save=True):
    with st.chat_message(role):  # Streamlitì—ì„œ ì±„íŒ… ë©”ì‹œì§€ ë°•ìŠ¤ë¥¼ ìƒì„±
        st.markdown(message)  # ë©”ì‹œì§€ ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
    if save:
        save_message(message, role)  # ì„¸ì…˜ ìƒíƒœì— ë©”ì‹œì§€ ì €ì¥

# ì´ì „ ë©”ì‹œì§€ ì´ë ¥ì„ UIì— ë‹¤ì‹œ ê·¸ë¦¬ëŠ” í•¨ìˆ˜
def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,  # ì´ë¯¸ ì €ì¥ëœ ë©”ì‹œì§€ëŠ” ë‹¤ì‹œ ì €ì¥í•˜ì§€ ì•ŠìŒ
        )

# ë¬¸ì„œ ë‚´ìš©ì„ í¬ë§·í•˜ëŠ” í•¨ìˆ˜ (í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)
def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

# ëŒ€í™”ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.

            Context: {context}
            """,
        ),
        ("human", "{question}"),
    ]
)

# Streamlit ì•±ì˜ UI ì„¤ì •
st.title("DocumentGPT")

st.markdown(
    """
Welcome!

Use this chatbot to ask questions to an AI about your files!

Upload your files on the sidebar.
"""
)

# ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ ì—…ë¡œë“œ UI
with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )

# íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš°
if file:
    retriever = embed_file(file)  # íŒŒì¼ ì„ë² ë”© í›„ ê²€ìƒ‰ê¸° ìƒì„±
    send_message("I'm ready! Ask away!", "ai", save=False)  # ì¤€ë¹„ ì™„ë£Œ ë©”ì‹œì§€ í‘œì‹œ
    paint_history()  # ê¸°ì¡´ ë©”ì‹œì§€ ì´ë ¥ í‘œì‹œ

    message = st.chat_input("Ask anything about your file...")  # ì‚¬ìš©ì ì…ë ¥ ë°›ìŒ
    if message:
        send_message(message, "human")  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ í‘œì‹œ
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),  # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
                "question": RunnablePassthrough(),  # ì‚¬ìš©ì ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì „ë‹¬
            }
            | prompt  # ì§ˆë¬¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
            | llm  # ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì‘ë‹µ ìƒì„±
        )
        with st.chat_message("ai"):  # AI ì‘ë‹µ ë°•ìŠ¤ ìƒì„±
            chain.invoke(message)  # ì–¸ì–´ ëª¨ë¸ ì‹¤í–‰ ë° ì‘ë‹µ í‘œì‹œ

# íŒŒì¼ì´ ì—†ì„ ê²½ìš° ë©”ì‹œì§€ ì´ˆê¸°í™”
else:
    st.session_state["messages"] = []

